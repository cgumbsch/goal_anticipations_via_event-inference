{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import interaction_gym\n",
    "import numpy as np\n",
    "import event_inference as event\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for this particular run, all seeding is done when initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational model we use here learns probabilistic event schema-representations. All schemata are composed of three probability distributions:\n",
    "- a starting condition, encoding which kind of observations typically result in an activation of this schema\n",
    "- a dynamics model, which encodes how the observations typically change during on even\n",
    "- a ending condition, which encodes what kind of observation is typically required for this event to end\n",
    "\n",
    "All three components are modeled as Gaussian distributions that live in the space of observations. \n",
    "\n",
    "Two examples of what these schemata could potentially encode for the events 'reaching' and 'falling':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Doc/schema1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Doc/schema2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model distinguishes between training and testing phases. During training the system learns the schemata in a supervised fashion, i.e., it receives explicit labels about the ongoing event. During testing, on the other hand, the system has to use the learned distributions, to infer the probability of on event being active, given the perceived sensorimotor information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = event.CAPRI(epsilon_start=0.01, epsilon_dynamics=0.001, epsilon_end=0.001,\n",
    "                    no_transition_prior=0.9, dim_observation=18, num_policies=3, \n",
    "                    num_models=4, r_seed=seed, sampling_rate=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the system in a simple agent-patient interaction simulation. In this simulation multiple event sequences $E$ can be observed. Each sequence $E$ is composed of multiple events $e_i$. The following table shows the possible vent sequences and their event components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Doc/events.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system can interact with the environment by different gaze policies $\\pi$. The gaze policy states where the system decides to look. Depending on the gaze, the system receives clear or noisy sensory information about the agent and patient. The gaze position is visualized in simulation as a small red dot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = interaction_gym.InteractionEventGym(sensory_noise_base=1.0, sensory_noise_focus=0.01, randomize_colors = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the sensorimotor loop. During training the system is trained on 100 event sequences. For each sequence a random policy $\\pi$ is determined. The system receives a new observation in every time step and updates it event schemata. \n",
    "\n",
    "env.render() visualizes the simulation. If this line is removed, no rendering takes place and the simulation runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episodes in range(3000):\n",
    "    \n",
    "    # Reset environment to new event sequence\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # Sample one-hot-encoding of policy pi(0)\n",
    "    policy_t = np.array([0.0, 0.0, 0.0])\n",
    "    policy_t[random.randint(0, 2)] = 1\n",
    "    for t in range(3000):\n",
    "        \n",
    "        #Rendering if desired:\n",
    "        #env.render()#store_video=True, video_identifier=0)\n",
    "        \n",
    "        # Perform pi(t) and receive new observation o(t)\n",
    "        observation, reward, done, info = env.step(policy_t)\n",
    "        \n",
    "        # Update the event probabilities, event schemata, and infer next policy\n",
    "        policy_t, P_ei = model.step(o_t=observation, pi_t=policy_t, training=True, done=done, e_i=info)\n",
    "        \n",
    "        # Next sequence when event sequence is over\n",
    "        if done:\n",
    "            print(\"Episode \", episodes, \" done after \", t , \" time steps\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the system:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training phase is followed by a testing phase. Our testing phases are inspired by studies of goal-prediction in infants. Here infants observe reaching movements done by a hand or a mechanical claw. Their gaze is tracked to determine if they are able to anticipate the action goal. If the infant looks at the target of reaching before the agent actually reaches it, it is considered a goal-predictive gaze.\n",
    "\n",
    "There are various experimental findings on this goal-predictive gaze. Apparently, young infants that have little experience in grasping never perform a goal-predictive gaze. 11-month-old infants perform a goal-predictive gaze when the event is performed by a familiar agent (hand) but not if it is performed by an unfamiliar agent (claw).\n",
    "\n",
    "Our testing phases also show reach-grasp-and-carry motions done by a hand or claw. Note, that only hand-agents perform reaching and transporting during training. \n",
    "\n",
    "Our hypothesis is, that if our model attempts to minimize uncertainty about future events and event boundaries and chooses its gaze accordingly, an anticipatory gaze behavior can emerge similar to the one of infants. However, the system can only anticipate the goal if it is able to identify the event. This is unlikely if the system has little experience with reaching (young age of infants / few training phases) or an agent is performing this event that was never observed before (claw-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Doc/hypothesis.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the testing phase 10 times with event and policy inference of the system. Here, the agent is a claw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodes in range(10):\n",
    "    \n",
    "    # Reset environment to new event sequence\n",
    "    observation = env.reset_to_grasping(claw=False) # claw=False for hand-agent\n",
    "    \n",
    "    # Sample one-hot-encoding of policy pi(0)\n",
    "    policy_t = np.array([0.0, 0.0, 0.0])\n",
    "    #policy_t[random.randint(0, 2)] = 1\n",
    "    policy_t[2] = 1.0\n",
    "    for t in range(3000):\n",
    "        #policy_t = np.array([0.0, 0.0, 1.0])\n",
    "        #Rendering if desired:\n",
    "        env.render() #store_video=True, video_identifier=0)\n",
    "        \n",
    "        # Perform pi(t) and receive new observation o(t)\n",
    "        observation, reward, done, info = env.step(policy_t)\n",
    "        \n",
    "        # Update the event probabilities, event schemata, and infer next policy\n",
    "        policy_t, P_ei = model.step(o_t=observation, pi_t=policy_t, training=False, done=done, e_i=info)\n",
    "        print(\"Event = \", info)\n",
    "        print(\"P_ei = \", P_ei)\n",
    "        print(\"Policy = \", policy_t)\n",
    "        \n",
    "        # Next sequence when event sequence is over\n",
    "        if done:\n",
    "            print(\"Episode \", episodes, \" done after \", t , \" time steps\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
